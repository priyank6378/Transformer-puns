{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 18:50:05.612152: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 18:50:05.612193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 18:50:05.613587: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 18:50:05.620015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 18:50:07.093202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Did you hear about the Native American man that drank 200 cups of tea?'\n",
      "  '[start] He nearly drown in his own tea pee. [end]']\n",
      " [\"What's the best anti diarrheal prescription?\"\n",
      "  '[start] Mycheexarphlexin [end]']]\n"
     ]
    }
   ],
   "source": [
    "# JOKES DATASET PREPARATION\n",
    "data = []\n",
    "with open(\"jokes.csv\") as f:\n",
    "    x = csv.reader(f)\n",
    "    f = 0;\n",
    "    for i in x:\n",
    "        if f:\n",
    "            data.append([\n",
    "                i[1], \n",
    "                '[start] ' + i[2] + ' [end]'\n",
    "            ])\n",
    "        f = 1;\n",
    "data = np.array(data )\n",
    "print(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What's 11 & 2?\" '[start] The Cowboys [end]']\n",
      "(38269, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data[120])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 128\n",
    "DENSE_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_HEADS = 8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 18:50:09.281271: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.323136: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.323598: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.324861: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.325236: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.325577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.422971: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.423208: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.423399: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-07 18:50:09.423555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2795 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\") \n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(s):\n",
    "    lowercase = tf.strings.lower(s)\n",
    "    return tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '')\n",
    "\n",
    "question_vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "answer_vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "question_data = data[:, 0]\n",
    "answer_data = data[:, 1]\n",
    "question_vectorizer.adapt(question_data)\n",
    "answer_vectorizer.adapt(answer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you hear about the Native American man that drank 200 cups of tea?\n",
      "[start] He nearly drown in his own tea pee. [end]\n",
      "38269\n",
      "38269\n"
     ]
    }
   ],
   "source": [
    "print(question_data[0])\n",
    "print(answer_data[0])\n",
    "print(len(question_data))\n",
    "print(len(answer_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100) (None, 101)\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(questions, answers):\n",
    "    tokenized_questions = question_vectorizer(questions)\n",
    "    tokenized_answers = answer_vectorizer(answers)\n",
    "    print(tokenized_questions.shape, tokenized_answers.shape)\n",
    "    x = {\"question\": tokenized_questions, \"answer\": tokenized_answers[:, :-1]}\n",
    "    y = tokenized_answers[: , 1:]\n",
    "    return (x, y)\n",
    "\n",
    "full_data_ds = tf.data.Dataset.from_tensor_slices((question_data, answer_data))\n",
    "full_data_ds = full_data_ds.batch(BATCH_SIZE)\n",
    "full_data_ds = full_data_ds.shuffle(buffer_size=4096)\n",
    "full_data_ds = full_data_ds.map(prepare_dataset, num_parallel_calls=8)\n",
    "full_data_ds = full_data_ds.prefetch(16).cache()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100)\n",
      "(16, 100)\n",
      "(16, 100)\n",
      "tf.Tensor(\n",
      "[   4    3   42  676   24  552   10 5400    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[   2 1123    3    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[1123    3    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 18:50:11.692956: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for i in full_data_ds.take(1):\n",
    "    print(i[0]['question'].shape)\n",
    "    print(i[0]['answer'].shape)\n",
    "    print(i[1].shape)\n",
    "    print(i[0]['question'][0])\n",
    "    print(i[0]['answer'][0])\n",
    "    print(i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove from memory\n",
    "del data\n",
    "del question_data\n",
    "del answer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='relu'),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dense_dim': self.dense_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim       \n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='relu'),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.support_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dense_dim': self.dense_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i>=j, dtype='int32')\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype='int32'\n",
    "            )\n",
    "            padding_mask = tf.minimum(padding_mask,  causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query = inputs,\n",
    "            value = inputs, \n",
    "            key = inputs,\n",
    "            attention_mask = causal_mask\n",
    "        )\n",
    "        attention_output_1 = self.layernorm_1(\n",
    "            attention_output_1 + inputs\n",
    "        )\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query = attention_output_1,\n",
    "            value = encoder_outputs,\n",
    "            key = encoder_outputs,\n",
    "            attention_mask = padding_mask\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_2 + attention_output_1\n",
    "        )\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(proj_output + attention_output_2)\n",
    "\n",
    "    \n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " question (InputLayer)       [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " answer (InputLayer)         [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " positional_embedding (Posi  (None, None, 128)            2572800   ['question[0][0]']            \n",
      " tionalEmbedding)                                                                                 \n",
      "                                                                                                  \n",
      " positional_embedding_1 (Po  (None, None, 128)            2572800   ['answer[0][0]']              \n",
      " sitionalEmbedding)                                                                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 128)            659712    ['positional_embedding[0][0]']\n",
      " formerEncoder)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_decoder (Trans  (None, None, 128)            1187456   ['positional_embedding_1[0][0]\n",
      " formerDecoder)                                                     ',                            \n",
      "                                                                     'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, None, 128)            0         ['transformer_decoder[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, None, 20000)          2580000   ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9572768 (36.52 MB)\n",
      "Trainable params: 9572768 (36.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype='int64', name='question')\n",
    "x = PositionalEmbedding(MAX_SEQUENCE_LENGTH, MAX_TOKENS, EMBEDDING_DIM)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(EMBEDDING_DIM, DENSE_DIM, NUM_HEADS)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype='int64', name='answer')\n",
    "x = PositionalEmbedding(MAX_SEQUENCE_LENGTH, MAX_TOKENS, EMBEDDING_DIM)(decoder_inputs)\n",
    "x = TransformerDecoder(EMBEDDING_DIM, DENSE_DIM, NUM_HEADS)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(MAX_TOKENS, activation='softmax')(x)\n",
    "\n",
    "transformer = keras.Model(\n",
    "    inputs = [encoder_inputs, decoder_inputs],\n",
    "    outputs = decoder_outputs\n",
    ")\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save('training_checkpoint/puns_bot.keras')\n",
    "        # with open(\"training_checkpoint/puns_bot.pickle\", 'wb') as f:\n",
    "        #     pickle.dump(transformer, f)\n",
    "\n",
    "callbacks = [SaveModelCallback()]\n",
    "\n",
    "transformer.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.3889 - accuracy: 0.9443\n",
      "Epoch 2/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.3637 - accuracy: 0.9462\n",
      "Epoch 3/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.3420 - accuracy: 0.9480\n",
      "Epoch 4/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.3238 - accuracy: 0.9497\n",
      "Epoch 5/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.3116 - accuracy: 0.9508\n",
      "Epoch 6/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2963 - accuracy: 0.9524\n",
      "Epoch 7/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2840 - accuracy: 0.9536\n",
      "Epoch 8/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2739 - accuracy: 0.9546\n",
      "Epoch 9/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2648 - accuracy: 0.9555\n",
      "Epoch 10/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2565 - accuracy: 0.9564\n",
      "Epoch 11/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2490 - accuracy: 0.9572\n",
      "Epoch 12/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2423 - accuracy: 0.9580\n",
      "Epoch 13/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2357 - accuracy: 0.9588\n",
      "Epoch 14/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2298 - accuracy: 0.9595\n",
      "Epoch 15/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2245 - accuracy: 0.9601\n",
      "Epoch 16/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2188 - accuracy: 0.9608\n",
      "Epoch 17/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2146 - accuracy: 0.9614\n",
      "Epoch 18/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2096 - accuracy: 0.9620\n",
      "Epoch 19/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2051 - accuracy: 0.9627\n",
      "Epoch 20/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.2008 - accuracy: 0.9632\n",
      "Epoch 21/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1968 - accuracy: 0.9637\n",
      "Epoch 22/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1927 - accuracy: 0.9643\n",
      "Epoch 23/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1895 - accuracy: 0.9648\n",
      "Epoch 24/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1870 - accuracy: 0.9651\n",
      "Epoch 25/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1824 - accuracy: 0.9657\n",
      "Epoch 26/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1794 - accuracy: 0.9663\n",
      "Epoch 27/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1759 - accuracy: 0.9667\n",
      "Epoch 28/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1730 - accuracy: 0.9671\n",
      "Epoch 29/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1698 - accuracy: 0.9675\n",
      "Epoch 30/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1667 - accuracy: 0.9680\n",
      "Epoch 31/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1638 - accuracy: 0.9685\n",
      "Epoch 32/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1614 - accuracy: 0.9688\n",
      "Epoch 33/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1591 - accuracy: 0.9691\n",
      "Epoch 34/50\n",
      "2392/2392 [==============================] - 183s 76ms/step - loss: 0.1561 - accuracy: 0.9697\n",
      "Epoch 35/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1537 - accuracy: 0.9700\n",
      "Epoch 36/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1516 - accuracy: 0.9703\n",
      "Epoch 37/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1495 - accuracy: 0.9706\n",
      "Epoch 38/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1468 - accuracy: 0.9710\n",
      "Epoch 39/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1452 - accuracy: 0.9712\n",
      "Epoch 40/50\n",
      "2392/2392 [==============================] - 183s 76ms/step - loss: 0.1424 - accuracy: 0.9717\n",
      "Epoch 41/50\n",
      "2392/2392 [==============================] - 183s 76ms/step - loss: 0.1408 - accuracy: 0.9720\n",
      "Epoch 42/50\n",
      "2392/2392 [==============================] - 183s 76ms/step - loss: 0.1389 - accuracy: 0.9722\n",
      "Epoch 43/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1366 - accuracy: 0.9726\n",
      "Epoch 44/50\n",
      "2392/2392 [==============================] - 181s 76ms/step - loss: 0.1347 - accuracy: 0.9729\n",
      "Epoch 45/50\n",
      "2392/2392 [==============================] - 181s 76ms/step - loss: 0.1329 - accuracy: 0.9732\n",
      "Epoch 46/50\n",
      "2392/2392 [==============================] - 181s 76ms/step - loss: 0.1310 - accuracy: 0.9735\n",
      "Epoch 47/50\n",
      "2392/2392 [==============================] - 181s 76ms/step - loss: 0.1295 - accuracy: 0.9738\n",
      "Epoch 48/50\n",
      "2392/2392 [==============================] - 181s 76ms/step - loss: 0.1272 - accuracy: 0.9741\n",
      "Epoch 49/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1259 - accuracy: 0.9744\n",
      "Epoch 50/50\n",
      "2392/2392 [==============================] - 182s 76ms/step - loss: 0.1243 - accuracy: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f33ab56be50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(\n",
    "    full_data_ds, \n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] because he always made the droid he was looking for [end]\n"
     ]
    }
   ],
   "source": [
    "inp = \"Why did the storm trooper decide to buy an iPhone?\"\n",
    "inp = question_vectorizer([inp])\n",
    "\n",
    "target_dict = dict([(i, val) for i, val in enumerate(answer_vectorizer.get_vocabulary())])\n",
    "\n",
    "output = \"[start]\"\n",
    "for i in range(MAX_SEQUENCE_LENGTH):\n",
    "    x = answer_vectorizer([output])[:, :-1]\n",
    "    x = transformer([inp, x])\n",
    "    x = x[:, i, :]\n",
    "    x = tf.argmax(x, axis=-1)\n",
    "    x = target_dict[x.numpy()[0]]\n",
    "    output += \" \" + x\n",
    "    if x == \"[end]\":\n",
    "        break\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] he took a vow of pavarotti [end]\n"
     ]
    }
   ],
   "source": [
    "inp = \"What did Tom hanks do in the woods?\"\n",
    "inp = question_vectorizer([inp])\n",
    "\n",
    "target_dict = dict([(i, val) for i, val in enumerate(answer_vectorizer.get_vocabulary())])\n",
    "\n",
    "output = \"[start]\"\n",
    "for i in range(MAX_SEQUENCE_LENGTH):\n",
    "    x = answer_vectorizer([output])[:, :-1]\n",
    "    x = transformer([inp, x])\n",
    "    x = x[:, i, :]\n",
    "    x = tf.argmax(x, axis=-1)\n",
    "    x = target_dict[x.numpy()[0]]\n",
    "    output += \" \" + x\n",
    "    if x == \"[end]\":\n",
    "        break\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] its like butter baby [end]\n"
     ]
    }
   ],
   "source": [
    "inp = \"What do A Tribe Called Quest and margarine have in common?\"\n",
    "inp = question_vectorizer([inp])\n",
    "\n",
    "target_dict = dict([(i, val) for i, val in enumerate(answer_vectorizer.get_vocabulary())])\n",
    "\n",
    "output = \"[start]\"\n",
    "for i in range(MAX_SEQUENCE_LENGTH):\n",
    "    x = answer_vectorizer([output])[:, :-1]\n",
    "    x = transformer([inp, x])\n",
    "    x = x[:, i, :]\n",
    "    x = tf.argmax(x, axis=-1)\n",
    "    x = target_dict[x.numpy()[0]]\n",
    "    output += \" \" + x\n",
    "    if x == \"[end]\":\n",
    "        break\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] he was having a midlife crisis [end]\n"
     ]
    }
   ],
   "source": [
    "inp = \"Why did the three year old African boy buy a red convertible?\"\n",
    "inp = question_vectorizer([inp])\n",
    "\n",
    "target_dict = dict([(i, val) for i, val in enumerate(answer_vectorizer.get_vocabulary())])\n",
    "\n",
    "output = \"[start]\"\n",
    "for i in range(MAX_SEQUENCE_LENGTH):\n",
    "    x = answer_vectorizer([output])[:, :-1]\n",
    "    x = transformer([inp, x])\n",
    "    x = x[:, i, :]\n",
    "    x = tf.argmax(x, axis=-1)\n",
    "    x = target_dict[x.numpy()[0]]\n",
    "    output += \" \" + x\n",
    "    if x == \"[end]\":\n",
    "        break\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
